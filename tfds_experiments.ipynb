{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfds experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0vCZ4xfe16B0L+48jKCQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashsmehta/TensorFlow-Examples/blob/master/tfds_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAqpvs49jqnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WjU5Lo3j0AZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'tfds'\n",
        "\n",
        "# fetch full dataset and info for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# you can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "\n",
        "# get the dataset information first:\n",
        "_, dataset_info = tfds.load(name=\"mnist\", split='train[:1%]', batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "\n",
        "# compute dimensions using the dataset information\n",
        "num_classes = dataset_info.features['label'].num_classes\n",
        "height, width, channels = dataset_info.features['image'].shape\n",
        "num_pixels = height * width * channels\n",
        "\n",
        "# select which split of the data to use:\n",
        "trainsplit = 'train[:100%]'\n",
        "testsplit = 'test[:100%]'\n",
        "\n",
        "train_data = tfds.load(name=\"mnist\", split=trainsplit, batch_size=-1, data_dir=data_dir, with_info=False)\n",
        "train_data = tfds.as_numpy(train_data)\n",
        "\n",
        "# full train set:\n",
        "train_images = train_data['image']\n",
        "num_train = len(train_images)\n",
        "\n",
        "# compute essential statistics for the dataset on the full trainset:\n",
        "data_minval = train_images.min()\n",
        "data_mean = train_images.mean()\n",
        "data_maxval = train_images.max()\n",
        "data_stddev = train_images.std()\n",
        "\n",
        "# create a one-hot encoding of x of size k:\n",
        "def one_hot(x, k, dtype=np.float32):\n",
        "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
        "\n",
        "#standadize data to have 0 mean and unit standard deviation\n",
        "def standardize_data(x, data_mean, data_stddev):\n",
        "  return (x - data_mean)/data_stddev\n",
        "\n",
        "def prepare_data(x, y):\n",
        "    x = standardize_data(x, data_mean, data_stddev)\n",
        "    x = np.reshape(x, (len(x), num_pixels))\n",
        "    x = jnp.asarray(x)\n",
        "    y = one_hot(y, num_classes)\n",
        "    return x, y\n",
        "\n",
        "def get_rawdata_batches(batchsize=100, split='train[:100%]'):\n",
        "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
        "  ds = tfds.load(name='mnist', split=split, as_supervised=True, data_dir=data_dir)\n",
        "\n",
        "  # you can build up an arbitrary tf.data input pipeline\n",
        "  ds = ds.batch(batchsize).prefetch(1)\n",
        "\n",
        "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "# create a generator that normalizes the data and makes it into JAX arrays\n",
        "def get_data_batches(batchsize=100, split='train[:100%]'):\n",
        "    ds = get_rawdata_batches(batchsize, split)\n",
        "\n",
        "    # at the end of the dataset a 'StopIteration' exception is raised\n",
        "    try:\n",
        "        # keep getting batches until you get to the end.\n",
        "        while True:\n",
        "            x, y = next(ds)\n",
        "            # x, y = prepare_data(x, y)\n",
        "            yield (x, y)\n",
        "    except:\n",
        "        pass\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kMt9XhCkAsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e6bfea5-6c8e-4d99-8b78-e4892cac3c9b"
      },
      "source": [
        "for x, y in get_data_batches(batchsize=10, split='train[:100%]'):\n",
        "  print(y)\n",
        "  break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 1 0 7 8 1 2 7 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n4ROuAYkjf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "860dde29-7d9d-48b8-c1ae-9904858b8d60"
      },
      "source": [
        "for x, y in get_data_batches(batchsize=50, split='train[:100%]'):\n",
        "  print(y)\n",
        "  break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3 0 9 4 9 0\n",
            " 6 8 4 7 2 6 0 3 1 1 7 2 4]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}